# -*- coding: utf-8 -*-
"""loan-approval-prediction-using-ann.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRLRJZ9txXeyJq-U3syvJbwseh08EXp_

**Import Necessary Libraries**
"""

import numpy
import sklearn
from sklearn import preprocessing
import pandas as pd
import os
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
import joblib
from joblib import dump

"""**Read the dataset as a pandas Dataframe**"""

loan_data = pd.read_csv("D:/Advanced Machine Learining Project/Dataset/loan_approval_dataset.csv")
loan_data.head()

"""Label Encoders to persorm One-Hot Encoding for categorical columns"""

label_encoder = preprocessing.LabelEncoder()
loan_data.columns

loan_data.head()

label_encoder_y = preprocessing.LabelEncoder()

loan_data[' education'] = label_encoder.fit_transform(loan_data[' education'])
loan_data[' self_employed'] = label_encoder.fit_transform(loan_data[' self_employed'])
loan_data[' loan_status'] = label_encoder_y.fit_transform(loan_data[' loan_status'])
loan_data.head()

"""**Convert dataframe to Numpy**"""

np_loan = loan_data.to_numpy()
X_data = np_loan[:,1:12]
Y_data=np_loan[:,12]

#Scale the data to ease the computation and prevent loss
scaler = StandardScaler().fit(X_data)
X_data = scaler.transform(X_data)
Y_data = tf.keras.utils.to_categorical(Y_data,num_classes = 2)
joblib.dump(scaler, "scaler.pkl")

"""**Train Test split**"""

#Splitting data into train and test with a ratio of 80:20
X_train,X_test,Y_train,Y_test = train_test_split(X_data,Y_data,train_size = 0.8)
print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)

#Defining the model, you can add or delete layers here and change the parameters to understand how it affects accuracy.

model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(512,input_shape = (11,),name = "Hidden_layer_1",activation="relu"))
model.add(keras.layers.Dense(512,name = "Hidden_layer_2",activation="relu"))
#model.add(keras.layers.Dense(16,name = "Hidden_layer_3",activation="relu"))
model.add(keras.layers.Dense(128,name = "Hidden_layer_4",activation="relu"))
model.add(keras.layers.Dense(2,name = "Output_layer",activation="softmax"))
model.compile(loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

"""Defining arguements for training the model  """

verbose = 1
batch_size = 8
epochs = 30
validation_split = 0.1

"""**Training the model**"""

history=model.fit(X_train,
          Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=verbose,
          validation_split=validation_split)

"""Observing the training curve"""

import matplotlib.pyplot as plt

pd.DataFrame(history.history)["accuracy"].plot(figsize=(8, 5))
plt.title("Accuracy improvements with Epoch")
plt.show()

print("\nEvaluation against Test Dataset :\n")
model.evaluate(X_test,Y_test)

"""**Saving the model for further use**"""

#model.save("loan_seq")
model.export("loan_seq")
model.save("loan_seq.h5")
dump(model, "ANN.joblib")

prediction_input = [[0,1,1,4100000,12200000,8,417,2700000,2200000,8800000,3300000]]

#Scale prediction data with the same scaling model
scaled_input = scaler.transform(prediction_input)
prediction = model.predict(scaled_input)
print(prediction)

"""**Evaluating accuracy against test data**"""

pred = np.argmax(prediction)
print("Prediction is ", label_encoder_y.inverse_transform([pred]))

prediction_input = [[5,1,1,8000000,27200000,6,348,6400000,300000,16700000,7100000]]

#Scale prediction data with the same scaling model
scaled_input = scaler.transform(prediction_input)
prediction = model.predict(scaled_input)
print(prediction)

pred = np.argmax(prediction)
print("Prediction is ", label_encoder_y.inverse_transform([pred]))

loan_data.head()