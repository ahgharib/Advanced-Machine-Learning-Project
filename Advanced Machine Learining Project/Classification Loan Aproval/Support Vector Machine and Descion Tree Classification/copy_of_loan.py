# -*- coding: utf-8 -*-
"""Copy of LOAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFTUwKQp05BIaUPzI7Rb3vwcaloyHQ7x
"""

import numpy as np
import pandas as pd
import pydotplus
import seaborn as sns
from sklearn.tree import export_graphviz
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_predict, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.feature_selection import RFE
import sklearn.metrics as metrics
from sklearn.metrics import roc_curve, roc_auc_score,precision_recall_curve
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,mean_squared_error

from io import StringIO
from IPython.display import Image, display
from sklearn import tree
from sklearn.tree import export_graphviz, DecisionTreeRegressor, DecisionTreeClassifier


from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, loguniform
from joblib import dump
import joblib

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("D:/Advanced Machine Learining Project/Dataset/loan_approval_dataset.csv")
df

df.isnull().sum()

df.duplicated().sum()

df.drop('loan_id',axis=1 ,inplace=True)

df.dtypes

# Removing whitespaces from column names
df.columns = df.columns.str.strip()

# Removing white spaces from values in the dataframe
df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)

df.self_employed.value_counts()

df.education.value_counts()

df.loan_status.value_counts()

# to convert object to int
df["self_employed"] = df["self_employed"].apply(lambda x: 1 if x == "Yes" else 0)
df["education"] = df["education"].apply(lambda x: 1 if x == "Graduate" else 0)
df["loan_status"] = df["loan_status"].apply(lambda x: 1 if x == "Approved" else 0)

df["loan_status"].unique()

df.head()

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='income_annum', y='loan_amount', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

df.info()

df

x=df.drop(['loan_status'], axis=1)
y=df['loan_status']

np.unique(y)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model_path = "scaler.joblib"
dump(scaler, model_path)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

svc = SVC(kernel="linear")

svc.fit(X_train_scaled, y_train)

y_pred = svc.predict(X_test_scaled)

print("Accuracy:", accuracy_score(y_test, y_pred))
# Generate a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
print(classification_report(y_test, y_pred))

#Finding the best parameters for Supported Vector classifiar model using grid_search
param_grid = {
    'C': [1, 10, 100],  # Penalty parameter C
    'gamma': [1, 0.1, 0.01],  # Kernel coefficient for 'rbf'
    'kernel': ['rbf','linear' ,'sigmoid']  # Kernel type
}

# Grid Search Cross Validation to find the best hyperparameters
grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_scaled, y_train)

# Best hyperparameters
best_params_grid = grid_search.best_params_
print("Best hyperparameters:", best_params_grid)

# Train SVM model with best hyperparameters
best_svm_grid = SVC(**best_params_grid)
best_svm_grid.fit(X_train_scaled, y_train)

# Evaluate the model
accuracy = best_svm_grid.score(X_test_scaled, y_test)
print("Accuracy on test set:", accuracy)

# Define the distribution of hyperparameters for Randomized Search
param_distributions = {
    'C': loguniform(1e-4, 1e4),  # Penalty parameter C (log scale)
    'gamma': loguniform(1e-4, 1e4),  # Kernel coefficient for 'rbf' (log scale)
    'kernel': ['rbf','sigmoid']  # Kernel type
}

# Randomized Search Cross Validation to find the best hyperparameters
random_search = RandomizedSearchCV(estimator=svc, param_distributions=param_distributions, n_iter=100,
                                   cv=5, n_jobs=-1, verbose=2, random_state=42)
random_search.fit(X_train_scaled, y_train)

# Best hyperparameters
best_params_rdm = random_search.best_params_
print("Best hyperparameters:", best_params_rdm)

# Train SVC model with best hyperparameters
best_svc_rdm = SVC(**best_params_rdm)
best_svc_rdm.fit(X_train_scaled, y_train)

# Evaluate the model
accuracy = best_svc_rdm.score(X_test_scaled, y_test)
print("Accuracy on test set:", accuracy)

from joblib import dump

model_path = "random_search_svm.joblib"
dump(random_search, model_path)

y_scores = cross_val_predict(svc, X_train_scaled, y_train, cv=5,method='decision_function')
fpr, tpr, thresholds = roc_curve(y_train,y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.figure(figsize=(12,8))
    plt.title('ROC curve')
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0,1],[0,1],"k--")
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')

plot_roc_curve(fpr,tpr)
plt.show()

y_scores = cross_val_predict(best_svm_grid, X_train_scaled, y_train, cv=5,method='decision_function')
fpr, tpr, thresholds = roc_curve(y_train,y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.figure(figsize=(12,8))
    plt.title('ROC curve')
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0,1],[0,1],"k--")
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')

plot_roc_curve(fpr,tpr)
plt.show()

y_scores = cross_val_predict(best_svc_rdm, X_train_scaled, y_train, cv=5,method='decision_function')
fpr, tpr, thresholds = roc_curve(y_train,y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.figure(figsize=(12,8))
    plt.title('ROC curve')
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0,1],[0,1],"k--")
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')

plot_roc_curve(fpr,tpr)
plt.show()

#we need to make a function that finds out the score of our training and testing
def M_S(y_real, y_pred, label):
    return pd.Series({'accuracy':accuracy_score(y_real, y_pred),
                      'precision': precision_score(y_real, y_pred,),
                      'recall': recall_score(y_real, y_pred,),
                      'f1': f1_score(y_real, y_pred,)},
                      name=label)

#we need to know The error on the training and test data sets
y_train_pred = svc.predict(X_train_scaled)
y_test_pred = svc.predict(X_test_scaled)

train_test_full_error = pd.concat([M_S(y_train, y_train_pred, 'train'),M_S(y_test, y_test_pred, 'test')],axis=1)
train_test_full_error

#we need to know The error on the training and test data sets
y_train_pred = best_svm_grid.predict(X_train_scaled)
y_test_pred = best_svm_grid.predict(X_test_scaled)

train_test_full_error = pd.concat([M_S(y_train, y_train_pred, 'train'),M_S(y_test, y_test_pred, 'test')],axis=1)
train_test_full_error

y_train_pred = best_svc_rdm.predict(X_train_scaled)
y_test_pred = best_svc_rdm.predict(X_test_scaled)

train_test_full_error = pd.concat([M_S(y_train, y_train_pred, 'train'),M_S(y_test, y_test_pred, 'test')],axis=1)
train_test_full_error

from sklearn.tree import DecisionTreeClassifier

# Create a DecisionTreeClassifier instance
decision_tree = DecisionTreeClassifier(random_state=42)

# Train the decision tree model
decision_tree.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = decision_tree.predict(X_test_scaled)

# Generate a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

model_path = "decision_tree.joblib"
dump(random_search, model_path)

# Plot the confusion matrix

print("Accuracy:", accuracy_score(y_test, y_pred))

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print evaluation metrics separately
print("Decision Tree Classifier Metrics:")
print(classification_report(y_test, y_pred))

y_train_pred = decision_tree.predict(X_train_scaled)
y_test_pred = decision_tree.predict(X_test_scaled)

train_test_full_error = pd.concat([M_S(y_train, y_train_pred, 'train'),M_S(y_test, y_test_pred, 'test')],axis=1)
train_test_full_error

# Create an output destination for the file
dot_data = StringIO()

export_graphviz(decision_tree, out_file=dot_data, filled=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())

# View the decision tree image
filename = 'loan_tree.png'
graph.write_png(filename)
img = Image(filename=filename)
display(img)

y_scores = cross_val_predict(decision_tree, X_train_scaled, y_train, cv=5,method='predict_proba')
# Get decision scores for the positive class (class 1)
y_scores = y_scores[:, 1]
fpr, tpr, thresholds = roc_curve(y_train,y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.figure(figsize=(12,8))
    plt.title('ROC curve')
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0,1],[0,1],"k--")
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')

plot_roc_curve(fpr,tpr)
plt.show()